<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Test</title>

    <!-- Theme CSS -->
    <link href="base.css" rel="stylesheet">
    <link href="grid.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>
    <link href="https://fonts.googleapis.com/css?family=Kaushan+Script" rel="stylesheet">

</head>

<body>

<div class="container text-container">
  <div class="header">
    <!--
      <div class="top-nav">
        <a href="">About</a>
        <a href="">Work</a>
        <a href="blog.html">Blog</a>
        <a href="">Contact</a>
      </div>
      -->
    </div>


<!--
<div class="col-md-3">
<h4>Operating Systems</h4>
<ul class="side side-nav" style="list-style: none;margin-left:0;padding-left:0">
  <li><a href="overview.html">Overview</a></li>
  <li><a href="fsck-journaling.html">Fsck and Journaling</a></li>
  <li><a href="lfs.html">Log-structured File Systems</a></li>
  <li><a href="distributed-systems.html">Distributed Systems</a></li>
  <li><a href="sci.html">SCI Transactions</a></li>
</ul>
</div>
-->
<br>
<div class="col-md-8">
<h1 style="margin:0;padding:0"> Workflow Evaluation Guidebook </h1>
<h2 style="margin-top:0;padding-top:0"> The UX M&amp;I Team | April 2017 </h2>

<h3>SUMMARY</h3>

<p>This guidebook outlines the method used to conduct a usability expert review as implemented in the 2017 Mission Critical Workflows Initiative.  The method can be carried out by project teams to evaluate additional workflows for benchmarking purposes or to re-evaluate updated workflows.  This method uses a modified version of the <a>Heuristic evaluation</a> process, a <a>usability engineering</a> method for finding the usability problems in a user interface design so that they can be attended to as part of an iterative design process. Heuristic evaluation involves having a small set of evaluators examine the interface and judge its compliance with recognized usability principles (<a>Nielsen’s 10 Heuristics</a>).” (Nielsen and Molich, 1990; Nielsen 1994).  Read more about heuristic evaluation <a>here</a>.</p>

<p>The method consists of three steps:</p>

<ol>
  <li>Define: Define the workflow, write a realistic task scenario, record it</li>
  <li>Score: Aggregate findings and assign each heuristic a severity rating</li>
  <li>Audit: Compile insights and assemble annotated screenshots illustrating key points</li>
</ol>

<br><h3>STEP 1: Define workflow, write &amp; record task scenario</h3>

<p>This is the preparation step.</p>

<p>Write a task scenario that includes a common path through the workflow done by a normal user.  It can incorporate some error conditions or highlight some additional common paths.  A good task scenario is a short paragraph and describes a fictional user, but it is based on reality.</p>

<p>It may be easier to create a recording (using SnagIt, Camtasia) for evaluation purposes, in that case narrate the task scenario from the point of view of the user in the scenario story.  Editing tools allow PHI to be blurred. In lieu of a recording, create a listing of all of the steps in sequence that the user would follow in athenaNet so that evaluators can replicate the scenario.  
</p>

<p>Example task scenario:</p>

<ul>
  <li>Workflow: Scheduling follow-up appointment</li>
  <li>User: Front desk worker</li>
  <li>Context: The front desk is a busy role, and moving through scheduling work accurately and efficiently is necessary to maintain workload and to ensure patients are not kept waiting.   Front desk users are frequently interrupted and also responsible for other types of tasks throughout the day.</li>
  <li>Task Scenario: Ted has just seen the provider for an ankle sprain and needs to return in two weeks to see how he’s doing. At check-out he tells the worker, Pam, that he needs an appointment in two weeks. Pam offers him a morning spot on which Ted agrees to. Just after Pam schedules the appointment Ted realizes that he has an early morning meeting that day and needs to come in the afternoon instead. To accommodate Ted, Pam double books a slot.</li>
</ul>

<br><h3>STEP 2: Aggregate findings and assign each heuristic a severity rating</h3>

<p>This is the “scorecarding” step.</p>

<ol>
  <li>Assemble 3-5 panelists who are familiar with heuristics and methodology</li>
  <li>Panelists individually watch recording of task scenario before the scoring session starts and write down individual findings (<a>References: Heuristic Guidebook, Printable Heuristic Notes Grid</a>)</li>
  <li>Panelists assemble for ~3 hours, watch the recording together and resolve any questions</li>
  <li>Findings are aggregated in a spreadsheet (tag each with heuristic and heuristic code from <a>Heuristic Guidebook</a>), and panel votes on severity ratings for each heuristic (References: <a>Heuristic Guidebook, Severity Scoring Key</a>)</li>
  <li>Average each of the 10 heuristic scores to obtain a composite score</li>
</ol>

<p>Scoring sessions should be conducted by a small panel of experts (3-5) whom are familiar with the heuristics and evaluation method.  We found that it works well when panel members individually preview the recording or replicate the task scenario steps prior to the scoring session.  Using the Heuristic Guidebook as reference, individuals log findings that they see during the preview session, and can use the Printable Heuristic Notes Grid to take notes.</p>

<p>The expert panel then assembles (approx 3 hours), views the video as a group, and begins to compile all of the individual findings.  We found that it is helpful to track the findings in a spreadsheet, where a scribe records a single, clean version of each finding and tags it with the heuristic and heuristic code.  Bucket each finding under the heuristic it’s associated with.  See the Heuristic Guidebook for examples of findings.  After compiling all findings for a single heuristic, the group votes on a severity score, discussing to reach consensus if needed.  Evaluators may invite a subject matter expert to the scoring session to answer questions and provide additional context as needed.</p>

<p>The individual scores for each of the 10 heuristics are averaged to obtain an overall composite score for the workflow.</p>

</div>
</div>

</body>
</html>
